# bigdata-customer-insights-pipeline
bigdata-customer-insights-pipeline

# ğŸ§  Customer Insights Big Data Pipeline

A full-scale big data engineering project designed to simulate a real-world analytics use case using AWS, Snowflake, PySpark, Redshift, Airflow, DBT, and more.

---

## ğŸš€ Project Objective

This project aims to build a scalable data pipeline that ingests and processes customer transactions and event logs, transforms the data into meaningful insights, and stores it in Redshift for analysis and reporting.

---


---

## ğŸ§‘â€ğŸ’» Getting Started

### Prerequisites

- Python 3.8+
- Docker & Docker Compose
- AWS Account
- Snowflake Account
- Git & GitHub

### Installation

###```bash
git clone https://github.com/<your-username>/bigdata-customer-insights-pipeline.git
cd bigdata-customer-insights-pipeline
pip install -r requirements.txt


## ğŸ§± Tech Stack

- **Python**, **PySpark**
- **AWS**: S3, Lambda, Glue, Athena, Redshift, Step Functions
- **Snowflake**: Source of transactional data
- **DBT**: Data transformation and testing on Redshift
- **Apache Airflow**: Workflow orchestration
- **GitHub Actions**: CI/CD pipeline
- **Docker**: Local development and testing

---

## ğŸ”„ Workflow Overview

1. Export transactional data from **Snowflake** to **S3**.
2. JSON logs (clickstream/cart events) are uploaded to **S3**.
3. **Lambda** triggers **Glue** to process and merge data.
4. Processed data is loaded into **Redshift**.
5. **DBT** builds data models for analytics.
6. **Airflow** orchestrates the entire pipeline.
7. Code is managed and deployed via **GitHub CI/CD**.

---

## ğŸ“‚ Repository Structure

bigdata-customer-insights-pipeline/
â”‚
â”œâ”€â”€ dags/                   # Airflow DAGs
â”œâ”€â”€ dbt/                    # DBT models and configs
â”‚   â”œâ”€â”€ models/
â”‚   â””â”€â”€ profiles.yml
â”œâ”€â”€ glue_jobs/              # PySpark scripts for Glue
â”œâ”€â”€ lambda_functions/       # Lambda handlers
â”œâ”€â”€ data/                   # Sample input data
â”‚   â”œâ”€â”€ snowflake_exports/
â”‚   â””â”€â”€ s3_json_logs/
â”œâ”€â”€ cicd/                   # GitHub Actions workflows
â”œâ”€â”€ terraform/              # (optional) infra as code
â”œâ”€â”€ notebooks/              # Jupyter notebooks (exploration, dev)
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ docker-compose.yml      # Local Airflow/DBT setup
â””â”€â”€ README.md               # Project overview


ğŸŒ Contributors
Your Name â€“ LinkedIn

Friend's Name â€“ LinkedIn

ğŸ“Œ License
This project is licensed under the MIT License.


